{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c7fcefa2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a672a",
   "metadata": {},
   "source": [
    "## Open-Ended Reflection Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8e1d4",
   "metadata": {},
   "source": [
    "### 1. Training Dynamics\n",
    "\n",
    "- Both the MLP and LSTM models show significant signs of **overfitting**, as training metrics reach near-perfection while validation loss begins to diverge and increase after the initial epochs. To address this, implementing **early stopping** after epoch 30 or increasing **regularization such as higher dropout rates or weight decay** would help generalize the model.\n",
    "- Because the Financial PhraseBank dataset is highly imbalanced, using `class_weight` prevents the models from simply guessing the majority neutral class to achieve high accuracy, and stabilizes training for minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP Training Curves](outputs/mlp_training_curves.png){#fig-mlp fig-align=\"center\"}\n",
    "\n",
    "![LSTM Training Curves](outputs/lstm_training_curves.png){#fig-lstm fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e0a3d",
   "metadata": {},
   "source": [
    "### 2. Model Performance and Error Analysis\n",
    "\n",
    "- The LSTM model generalized slightly better to the test set compared to the MLP. While both models show clear overfitting, the LSTM achieved a higher final Macro F1 Score 0.73+ and demonstrated a more robust diagonal in its confusion matrix.\n",
    "- The \"Negative\" classes were most frequently misclassified, often being mistaken for the \"Neutral\" class. This pattern occurs because the Financial PhraseBank dataset is highly imbalanced, with \"Neutral\" samples making up the vast majority of the data. Consequently, the models will develop a bias toward the \"Neutral\" class, making it difficult to distinguish nuanced sentiment from neutral reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06f552",
   "metadata": {},
   "source": [
    "::: {#fig-confusion-matrices layout-ncol=2}\n",
    "\n",
    "![MLP Confusion Matrix](outputs/mlp_confusion_matrix.png){#fig-mlp fig-align=\"center\"}\n",
    "\n",
    "![LSTM Confusion Matrix](outputs/lstm_confusion_matrix.png){#fig-lstm fig-align=\"center\"}\n",
    "\n",
    "Comparison of Confusion Matrices for MLP and LSTM Models\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec975f",
   "metadata": {},
   "source": [
    "### 3. Cross-Model Comparison\n",
    "\n",
    "- Mean-pooling word embeddings for the MLP forces the model to treat the sentence as \"a bag of words,\" which completely **ignores the word order and grammatical structure**. This limits the model's ability to distinguish between sentences containing the same words but expressing different sentiments through arrangement.\n",
    "- The LSTM's sequential architecture processes tokens one by one, allowing it to maintain a hidden state that **captures long-range dependencies** within the sentence. This enables the model to grasp the nuance of financial phrases, leading to better generalization and higher performance on the test set.\n",
    "- Yes, the fine-tuned LLMs significantly outperformed classical baselines. This is due to their large-scale pretraining on vast amounts of text, which provides them with a deep understanding of language logic. Unlike the static FastText embeddings used in MLP and LSTM, BERT and GPT generate contextual representations, meaning **the same word has a different vector based on its surrounding context**, allowing them to capture subtle financial nuances.\n",
    "- The general ranking of model performance in this case (from best to worst) is: BERT > GPT > GRU > LSTM > MLP > RNN. This is because classical models like MLP are hindered by static embeddings, while sequence-based models (RNN, LSTM, GRU) improve by adding temporal awareness, but still lack the deep semantic depth provided by the pretraining of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96ca61",
   "metadata": {},
   "source": [
    "::: {#fig-all-confusions layout-ncol=3}\n",
    "\n",
    "![MLP](outputs/mlp_confusion_matrix.png){#fig-mlp fig-align=\"center\"}\n",
    "\n",
    "![RNN](outputs/rnn_confusion_matrix.png){#fig-rnn fig-align=\"center\"}\n",
    "\n",
    "![LSTM](outputs/lstm_confusion_matrix.png){#fig-lstm fig-align=\"center\"}\n",
    "\n",
    "![GRU](outputs/gru_confusion_matrix.png){#fig-gru fig-align=\"center\"}\n",
    "\n",
    "![BERT](outputs/bert_confusion_matrix.png){#fig-bert fig-align=\"center\"}\n",
    "\n",
    "![GPT](outputs/gpt_confusion_matrix.png){#fig-gpt fig-align=\"center\"}\n",
    "\n",
    "Confusion Matrices Across All Six Models\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Use Disclosure (Required)\n",
    "\n",
    "If you used any AI-enabled tools (e.g., ChatGPT, GitHub Copilot, Claude, or other LLM assistants) while working on this assignment, you must disclose that use here. The goal is transparency-not punishment.\n",
    "\n",
    "In your disclosure, briefly include:\n",
    "- **Tool(s) used:** (name + version if known)\n",
    "- **How you used them:** (e.g., concept explanation, debugging, drafting code, rewriting text)\n",
    "- **What you verified yourself:** (e.g., reran the notebook, checked outputs/plots, checked shapes, read documentation)\n",
    "- **What you did *not* use AI for (if applicable):** (optional)\n",
    "\n",
    "You are responsible for the correctness of your submission, even if AI suggested code or explanations.\n",
    "\n",
    "---\n",
    "\n",
    "### In this assignment, I used AI to assist with:\n",
    "\n",
    "- Debugging library import errors and resolving version conflicts.\n",
    "- Explaining code and troubleshooting errors related to the torch library.\n",
    "- Assisting with Markdown formatting and layout for inserting images.\n",
    "- Refining and polishing language.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
